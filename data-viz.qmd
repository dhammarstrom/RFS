---
bibliography: references.bib
editor_options: 
  chunk_output_type: console
reference-location: margin
citation-location: margin
---

# Data visualization

The most apparent purpose of data visualizations in scientific contexts is to convey information not suited for text or tables but for graphical displays like comparisons between categories, relationships among variables, or trends over time [@gelman2002; @tufte2001]. We often think of the scientific graph as the end product of our labor. However, data visualization can be an effective tool for thinking about scientific problems and performing exploratory data analysis before preparing your manuscript. These steps will likely take up more time than polishing *Figure 1* of your upcoming paper.

Data visualization is thus a skill, like writing, with several purposes. You may write notes for yourself and write to communicate with others. Notes come in many forms, just as messages you write to others. Similarly, data visualizations can have different purposes, such as

-   Checking that your data is what could be expected

-   Diagnosing statistical models

-   Exploring a relationship between many variables

-   Showing that your measurements are reliable and valid

-   Conveying the key message from your study

Traditionally, and in many software implementations, data visualization is a matter of choosing from a menu of charts such as box plots, scatter plots, or line graphs. These can be very effective. However, you would not restrict your writing to pre-made templates. As a professional scientist in a quantitative genre, you need a toolbox for data visualizations that do not limit you. By using a *grammar of graphics* you can be more creative in designing your visualizations.

## A grammar of graphics - `ggplot2`

Building on Wilkinsons *The Grammar of Graphics* [@wilkinson2005], Wickham [@wickham2010; @wickham2016] implemented a graphical grammar to R through the package `ggplot2`. The idea behind both Wickhams and Wilkinsons formalized syntax for creating data visualizations is to avoid special cases or duplicate methods and allow for creation of data visualizations based on a set of core components used in all graphics [@wickham2010; @wilkinson2005].

The grammar (in the version described by Wickham [@wickham2010]) has seven components that together creates a ggplot2 visualization (@fig-ggplotlayers).

The underlying **data** contains the variables that are **mapped to aesthetics** such as coordinates, colors or shapes. Each aesthetic can be controlled through **scales** by assigning values to coordinates, colors, shapes etc. **Geometric objects** creates the visual representations of the mapping. Sometimes small multiples of the same graph are created using **facets** which creates subdivision of the data to be plotted in different panels. **Statistical transformations** creates summaries of the data, however, in its simplest form, the transformation is the identity transformation. All graphical representations are plotted in a **coordinate** system. **Theme and annotations** adds non-data layers to the plot which can include geometric shapes or text (@fig-ggplotlayers).

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: fig-ggplotlayers
#| fig-cap: "Components of a ggplot2 visualization"

library(tidyverse)

## Create a data frame for plotting a rectangle that is tilted to look like a plane in 3D
parallelogram_data <- data.frame(
  x = rep(c(1, 4, 6, 3), 7),
  y = rep(c(1.5, 1.5, 3, 3), 7),
  col = rep(c("a", "b", "c", "d", "e", "f", "g"), each = 4), 
  add = rep(c(1, 2, 3, 4, 5, 6, 7), each = 4)) %>%
  mutate(y = y + add)

# Plot the parallelogram using ggplot2
ggplot() +
  geom_polygon(data = parallelogram_data, aes(x = x, y = y, fill = col), color = "black", alpha = 0.3) +
  coord_fixed(ratio = 1) +  # This is to ensure that the x and y axis are on the same scale
  geom_text(data = data.frame(y = seq(1:7) + 2, 
                              lab = c("Data", "Aesthetics",
                                      "Geometric objects", "Facets", 
                                      "Statistical\ntransformations", 
                                      "Coordinates", "Theme and\n annotations")), 
            aes(x = 0.5, y = y, label = lab), 
            hjust = 1) +
  
  scale_x_continuous(limits = c(-2, 6)) +

  theme_void() + 
  theme(legend.position = "none")



```

### Building blocks of a plot

Using a similar example as in [@wickham2010] a basic plot can be broken down into the data (see @tbl-ggplotdataexample), geometric objects, scales and coordinates and annotations. These components, that can be individually manipulated, together creates the final plot (see @fig-ggplotexample a-d).

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: tbl-ggplotdataexample
#| tbl-cap: "Example data for a ggplot"


library(tidyverse); library(gt)

data.frame(x = c(1, 2.4, 4, 5.2), 
           y = c(3, 2, 5, 7.8), 
      
           shape = c("a", "a", "b", "b")) %>%
  gt()



```

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: fig-ggplotexample
#| fig-cap: "Three components, geometric objects (a), coordinates and scales (b) and plot annotations (c) are constitues of the complete plot (d)"
library(cowplot)

a <- data.frame(x = c(1, 2.4, 4, 5.2), 
           y = c(3, 2, 5, 7.8), 
           color = c("s", "m", "s", "m"), 
           shape = c("a", "a", "b", "b"), 
           size = c(2, 3, 4, 5)) %>%
  ggplot(aes(x, y, shape = shape)) + geom_point(size = 3) + 
  theme_void() +
  theme(legend.position = "none")
 
b <- data.frame(x = c(1, 2.4, 4, 5.2), 
           y = c(3, 2, 5, 7.8), 
           color = c("s", "m", "s", "m"), 
           shape = c("a", "a", "b", "b"), 
           size = c(2, 3, 4, 5)) %>%
  ggplot(aes(x, y, shape = shape))  + 
  theme_classic() +
  theme(legend.position = "none", 
        axis.text = element_blank(), 
        axis.title = element_blank())

c <- data.frame(x = c(1, 2.4, 4, 5.2), 
           y = c(3, 2, 5, 7.8), 
           color = c("s", "m", "s", "m"), 
           shape = c("a", "a", "b", "b"), 
           size = c(2, 3, 4, 5)) %>%
  ggplot(aes(x, y, shape = shape))  + 
  theme_classic() +
  geom_point(color = "white") + 
  labs(title = "Title", 
       subtitle = "Subtitle", 
       x = "x-axis", 
       y = "y-axis", 
       shape = "legend") +
  
  theme(axis.ticks = element_blank(), 
        axis.line = element_blank())


d <- data.frame(x = c(1, 2.4, 4, 5.2), 
           y = c(3, 2, 5, 7.8), 
           color = c("s", "m", "s", "m"), 
           shape = c("a", "a", "b", "b"), 
           size = c(2, 3, 4, 5)) %>%
  ggplot(aes(x, y, shape = shape))  + 
  theme_classic() +
  geom_point(color = "black", size = 3) + 
  labs(title = "Title", 
       subtitle = "Subtitle", 
       x = "x-axis", 
       y = "y-axis", 
       shape = "legend") +
  
  theme()


plot_grid(
    plot_grid(a, b, c, nrow = 1, align = "vh", 
              labels = c("a", "b", "c"), 
          rel_widths = c(0.8, 0.8, 1)), 
    plot_grid(NULL, d, NULL, rel_widths = c(0.7, 1, 0.7),
              nrow = 1,
              labels = c("", "d", "")), 
    nrow = 2)

```

The corresponding code to create this plot is fairly straight forward:

```{r}
#| eval: false
#| echo: true

library(ggplot2)

df <- data.frame(x = c(1, 2.4, 4, 5.2),  # <1> 
           y = c(3, 2, 5, 7.8), 
           shape = c("a", "a", "b", "b")) 


  ggplot(data = df,                      # <2>
         mapping = aes(x = x, y = y, shape = shape))  +    # <3>

  geom_point(color = "black", size = 3) + # <4>
  
    labs(title = "Title",        # <5>
       subtitle = "Subtitle", 
       x = "x-axis", 
       y = "y-axis", 
       shape = "legend") +
    theme_classic() # <6>


```

1.  A simple data frame is saved containing the variables we will plot
2.  The data is explicitly called
3.  Variables are mapped to coordinates (x and y) and shapes.
4.  Geometric objects (points) are specified with color and size determined in the function
5.  Annotations are added to the plot through the `labs` (labels) function.
6.  A pre-built theme is used to change the default appearance of the plot.





## Before plotting

As we saw above, a basic visualization can be created from a data set, or a data frame which is the most common representation of data in R. A *tidy* data set has one observation per row and one variable per column. A tidy data set makes data visualization easy. However, not all data sets are friendly. In fact, some might be unfriendly because they are unhappy[^unhappy].

::: {.column-margin}

[^unhappy]: The reference to happiness is a reference to @wickham2014 wherein Tolstoy's Anna Karenina is quoted; "Happy families are all alike; every unhappy family is unhappy in its own way." The Anna Karenina principle applies to data as non-tidy data can be non-tidy in many ways, tidy data however are tidy because they all share a common set of features.

:::

A lot of effort goes into making data sets suitable for visualization or statistical modelling. The good news is that R is especially suited for the process of importing and wrangling data. As with other common tasks in R there are numerous ways of achieving the same goals. This is a good thing because it allows for solutions to a wide range of problems. It is also a bad thing because it makes it difficult to getting started. A collection of R packages called the [Tidyverse](https://www.tidyverse.org/) makes the process of getting started with data wrangling easier. 

Tidyverse can be thought of as a dialect of the R language. The dialect is designed to make it easy to write sequential operations in a way that translates thoughts and ideas to code. Sequential operations are enabled by a pipe operator. Using a pipe operator we can call functions in sequential order to do specific operations on the data. We can write such a pipe as demonstrated below with the corresponding English language descriptions to the left. 


:::: {.columns}

::: {.column width="60%"}

Take the data ***then do***<br>
filter the data based on *x* larger than 10 ***then do***<br>
add a new calculated variable z = x + y ***then do***<br>
show the output<br>

:::

::: {.column width="40%"}


```{r}
#| echo: true
#| eval: false

data |>
  filter(x > 10) |>
  mutate(z = x + y) |>
  print()


```


:::

::::

The pipe operator (`|>`) takes any input and place it as the first argument in the following function[^funnote]. This is the mechanism that makes sequential operations possible. A pipe operator makes code more readable, consider the following two alternatives:

::: {.margin-column}

[^funnote]: A function is a basic building block of your R code. Function are designed for specific tasks and can be part of packages or created by the user. A function may take arguments such as `fun(<ARGUMENT1> = <default input>, <ARGUMNET2> = <default input>)`. Arguments can be used without explicitly naming them by putting expected input in at the right position. Orders of argument may be changed is argument names are used.  

:::



```{r}
#| eval: false
#| echo: true

print(mutate(filter(select(data, var1:var3), var1 == "xxx"), var4 = var1 + var2)) # <1>


data |> # <2>
  select(var1:var3) |>
  filter(var1 == "xxx") |>
  mutate(var4 = var1 + var2) 


```

1. Alternative 1: A number of operations are performed on data, we have to read from in to out to see each step.
2. Alternative 2: The same steps are performed as in alternative 1, in the same order. 

The pipe in alternative 2 are structured with the pipe operator making the code more readable and easier to edit. Notice that the same functions are used in both cases. 

In R there are two main pipe operators. We have the "base R" pipe, `|>`. This pipe operator is included in the base installation of R and available without loading any packages on start up. A second pipe belongs to the `magritter` package. The `magritter` pipe (`%>%`) has the same basic functionality as the base pipe; the left hand input is inserted as the first argument in any right hand function. 

Sometimes you might want to place your input somewhere else than as the first argument in a right hand function. A placeholder can be used to indicate where you would like to place your input. The base pipe differs from the `magritter` pipe in what symbol indicates a placeholder. In the example below, the function `some_fun()` expects data as the third argument. We need to use our placeholder to put the data in the correct position:


```{r}
#| eval: false
#| echo: true

data |> # <1>
  some_fun(arg1 = c(1, 2), 
           arg2 = "some.setting", 
           arg3 = _)


library(tidyverse)
data %>% # <2>
  some_fun(arg1 = c(1, 2), 
           arg2 = "some.setting", 
           arg3 = .)



```

1. With the base R forward pipe operator.
2. With the `magritter` forward pipe operator. 





### Reading data into R

Three packages makes reading tabular data into R easy. `readr` provides functions for reading and writing delimiter separated files, such as `.csv` or `.tsv`. `readxl` provides functions that imports data from excel files. An finally, `googlesheets4` makes it possible to read tabular data created in google sheets.


Data can also be loaded from packages in R since storing data is a convenient way of sharing. By including data in a package you are nudged to do some quality checks and document it. We will talk more about data packages in a later workshop. 



### The verbs of data wrangling

Once data is available in our workspace we will be able to wrangle it. In the examples below I will use a data set containing results from dual x-ray absorptiometry measurements available in the `exscidata` package. To install the `exscidata` package:

```{r}
#| echo: true
#| eval: false

library(remotes)
install_github("dhammarstrom/exscidata")

```

The `dplyr`package provides a collection of verbs to facilitate wrangling. As mentioned above, "pipeable" functions takes a data frame as its first argument. This means that we can line up functions in sequential order using a pipe operator. In all verb functions, following the data argument follows a set of arguments that specifies what we wish to do with the data. The result of the operations performed by the function are returned as a data frame. 

`dplyr` contains the following main data verbs:

- `select`
- `rename`
- `relocate`
- `mutate`
- `filter`
- `arrange`
- `summarize` 

In addition, several helper function will aid our wrangling endevors.

`dplyr` is loaded as part of the `tidyverse` package:

```{r}
#| echo: true
#| eval: false
#| message: false

library(tidyverse)

```


#### Select, rename and relocate variables

Variables in a data frame may be selected and renamed. Such operation may have multiple purposes such as giving you a better overview of the data of interest or limiting what data to display in a table. Renaming can make life easier if the data set contains long variable names. 

Below we will store a subset of the data in a new data set. But we will first have a look at what column names are available:

```{r}

library(exscidata) # <1>
glimpse(exscidata::dxadata) # <2>

```

The data set contains `r dim(exscidata::dxadata)[1]` rows and `r dim(exscidata::dxadata)[2]` columns. The variables in the data set are described as part of the `exscidata` package and can be seen by typing `?dxadata` in the console.

We will work further with lean body mass data, these are variables starting with `lean.`. In addition we need variables describing observations like `participant`, `time`, `multiple`, `single`, `sex`, `include`, `height` and `weight`. To select these variables we can try a couple of different approaches. 
The `select` function can select variables by name. This means that we can simply list them: 


```{r}

library(exscidata) # <1>

exscidata::dxadata |> # <2> 
  select(participant, time, multiple, single) |> # <3>
  print() # <4>
  
```

1. Load the `exscidata` package
2. Retrieve the data from the exscidata package
3. select variables based on names
4. Print the resulting data frame.

The above means a lot of work writing all columns names in the select call. An alternative approach is to select variables based on the first and last variable in a sequence. This is possible by using the syntax `<first column>:<last column>`. 

```{r}

exscidata::dxadata |>  
  select(participant:weight) |> # <1>
  print() 
  
```

1. Selecting by a first and last column in a sequence.

We also like to have the lean body mass data included in our new data set. Since all variables containing lean body mass data starts with `lean.` we can use a helper function to select them. Two alternatives are possible:


```{r}


exscidata::dxadata |>  
  select(participant:weight, starts_with("lean.")) |> # <1>
  print() 


exscidata::dxadata |>  
  select(participant:weight, contains("lean.")) |> # <2>
  print() 


```

1. Using `starts_with` to select all columns that starts with `lean.`
2. Using `contains`to select all variables that contains `lean.`

There are other select helper functions such as `ends_with` and `matches` that work in a similar fashion as the above. `all_of` and `any_of` helps you select variables based on a vector of variables, `where` selects based on where a function of your choosing returns true. See `?select` for a complete list.

In a `select` call we can also rename variables using the syntax `<new name> = <old name>`. Let's say we want to select and rename `participant`:

```{r}


exscidata::dxadata |>  
  select(parti = participant, time:weight, starts_with("lean.")) |> 
  print() 



```

Notice how different ways of selecting variables can be combined in `select`.

The `rename` function makes it easy to rename variables without the need to select.


```{r}

exscidata::dxadata |>  
  select(participant:weight, starts_with("lean.")) |> 
  rename(parti = participant) |>
  print() 


```


If we want to change the order of variables in a data set we can specify the order in a select call, or use `relocate`

```{r}

exscidata::dxadata |>  
  select(participant:weight, starts_with("lean.")) |> 
  relocate(lean.whole) |>
  print() 

```

`relocate` puts the selected variable as the first column in the data set. If we want to specify the location we can use the arguiments `.before` or `.after`. 

```{r}

exscidata::dxadata |>  
  select(participant:weight, starts_with("lean.")) |> 
  relocate(lean.whole, .after = sex) |>
  print() 

```

#### Creating new variables 

`mutate` let's us create new variables in a data set. These can be a function of variables already in the data set or created from your input. 

Let's create a variable representing the percentage of lean mass to body mass.

```{r}

exscidata::dxadata |>  
  select(participant:weight, starts_with("lean.")) |> 
  mutate(rel_lean_whole = 100 * ((lean.whole/1000) / weight)) |>
  relocate(rel_lean_whole) |>
  print() 

```


### From wide to long and back again

The `dxadata` data set is not a tidy data set. It contains two variables (`single` and `multiple`) that indicates which leg has been training with low and moderate volume respectively [@hammarstrom2020]. Additionally, lean mass variables could be separated based on body half (right or left). To compare training volume we need to reformat the data set. We will start by making a smaller data set that indicate training volume per leg. 

As we have already seen, `participant`, `single` and `multiple` are the variables needed to make a data set that indicates training volume per leg, per participant. We will start by selecting these columns followed by pivoting the data as the volume data is located in two variables. This essentially means that we will make the data set longer.  

```{r}

exscidata::dxadata |>  
  select(participant, multiple, single) |> # <1>
  pivot_longer(values_to = "leg", names_to = "volume",  cols = multiple:single) %>% # <2>
  print()
  


```

1. Selecting our variables of interest
2. Creating a long data set based on volume data spread over two columns.

As we see we now have a long data set, but it is longer than expected. The original data contains only `r length(pull(distinct(exscidata::dxadata, participant), participant))` participants. As each participant has two legs we would expect 82 observations. Above we did not remove post-intervention observations and we therefore have several duplicates. This can be taken care of by using the `distinct` function which returns unique observations across a combination of variables.


```{r}

participant_volume <- exscidata::dxadata |>  
  select(participant, multiple, single) |> # 
  pivot_longer(values_to = "leg", names_to = "volume",  cols = multiple:single) %>% # <2>
  distinct(participant, volume, leg) %>% # <1>
  print()
  
```

1. Removes all duplicate combinations of `participant`, `volume` and `leg`.

We can save our smaller data set as `participant_volume`

Next we want to create a data set of right and left leg lean mass data. We will start by selecting variables.

```{r}

exscidata::dxadata |>  
  select(participant, time, starts_with("lean.") & ends_with("_leg")) |> #
  print()



```

Notice how `&` was used to create a conditional selection of variables. I addition to selecting time, include[^incl] and participant we select variables that starts with `lean.` AND ends with `_leg`. 

::: {.column-margin}

[^incl]: The variable `incl` is used to indicate which participants to include in a final analysis. Included participants completed a given set of training sessions.

:::

The resulting data set is wide. Two variables contains the same variable (lean mass), but one variable (leg) is lurking in two variables (`lean.left_leg` and `lean.right_leg`). Let's make the data set long.


```{r}

leg_leanmass <- exscidata::dxadata |>   # <3>
  select(participant, time, include, starts_with("lean.") & ends_with("_leg")) |> 
  pivot_longer(names_to = "leg", values_to = "lean_mass", cols = contains("lean.")) |> # <1>
  mutate(leg = if_else(leg == "lean.left_leg", "L", "R")) |> # <2>
  
  print()



```

1. Notice how select helpers can be used in pivot_longer.
2. We use `mutate` together with `if_else` to change the leg indicator to `R` and `L`
3. We save the data set as `leg_leanmass`


`pivot_longer` has a brother called `pivot_wider`, this functions performs the reverse operation making long data sets wide. Let's say that we would like to calculate the paired difference of leag lean mass from pre to post, we could make a wider data set and calculate `post - pre`



```{r}

exscidata::dxadata |>  
  select(participant, time, include, starts_with("lean.") & ends_with("_leg")) |> 
  pivot_longer(names_to = "leg", values_to = "lean_mass", cols = contains("lean.")) |>
  mutate(leg = if_else(leg == "lean.left_leg", "L", "R")) |> 
  pivot_wider(names_from = time, values_from = lean_mass) |> # <1> 
  mutate(delta_lean_mass = post - pre) |> # <2>
  print()

```

1. Pivot wider creates new columns based on names in `time` and values in `lean_mass`
2. The new variables is calculated as the difference between pre and post-intervention values.

### Joining data sets
We have constructed two smaller data sets, one indicating which leg performed low and moderate volume and one data set containing the lean mass values for each leg, pre- and post-intervention. Next step is to join the two.

`dplyr` contains functions for joining data frames. There are, as illustrated in @fig-join-illustration important differences between the functions where outer joins (left, right and full) keeps all observations in `x`, `y` and both `x` and `y` respectively. `inner_join` however, drops unmatched observations from both input data frames. Unlike the others, `anti_join` function removes observations in `x` that is present in `y`.

```{r}
#| code-fold: true
#| code-summary: Code producing the figure
#| message: false
#| warning: false
#| label: fig-join-illustration
#| fig-cap: "Functions for joining data sets."


library(ggVennDiagram) 
library(ggplot2) 
library(cowplot)

# Joins can be illustrated with venn diagrams.
# We do not need information about sets etc only 
# a way to fill three distinct areas. 
# The ggVennDiagram contains a function creating 
# shapes to be filled.


lst <- process_data(Venn(list(x = c("1"), y = c("1"))))

# To avoid repeated code, create a function that plots
# venn diagrams with options to indicate a title and 
# specify fill colors
join_illustration <- function(lst, 
                              fills = c("steelblue", 
                                        "steelblue", 
                                        "white"), 
                              title = "left_join(x,y)") {
  
  ggplot() + 
    
  geom_sf(aes(fill = id), data = venn_region(lst), 
          color = "black") +
  
  theme_void() +
  # Hard coded labels
  geom_text(aes(x = c(300, 700), 
                y = c(500, 500), 
                label = c("x", "y")), 
            size = 8, 
            color = "gray20") +
  
  
  scale_fill_manual(values = fills) +
  
  labs(title = title) +
  
  theme(legend.position = "none")
  
  
}


# Each join gets its own fig. All figures are 
# ombined below with plot_grid from cowplot.

left <- join_illustration(lst = lst, 
                  title = "left_join(x,y)", 
                  fills = c("steelblue", "steelblue", "white"))
right <- join_illustration(lst = lst, 
                  title = "right_join(x,y)", 
                  fills = c("white", "steelblue", "steelblue"))
inner <- join_illustration(lst = lst, 
                  title = "inner_join(x,y)", 
                  fills = c("white", "steelblue", "white"))
full <- join_illustration(lst = lst, 
                  title = "full_join(x,y)", 
                  fills = c("steelblue", "steelblue", "steelblue"))
anti <- join_illustration(lst = lst, 
                  title = "anti_join(x,y)", 
                  fills = c("steelblue", "white", "white"))


plot_grid(left, right, inner, full, anti, ncol = 2)

```

Our two data sets are pretty insensitive to dropped observations since the two data sets should be complete. We will use a `left_join` to put the data set together. This will match participant and leg because these two variables exists in both data sets. If we want to join by other variables we may specify such a variable. We'll save the joined data set as `leg_leanmass`.

```{r}

leg_leanmass <- leg_leanmass |>
  inner_join(participant_volume) |>
  print()

```

### Filters and sorting rows

We included the variable `include` in our data set, this will make it possible to get rid of observations from participants that should be part of the final analysis. We can use `filter` to perform this operation.

```{r}
leg_leanmass |>
  filter(include == "incl") |>
  print()
```

The double equal sign can be read as "equal to" in R, a common source of confusion is that the single equal sign (`=`) is used as a assignment operator.[^assignop]

::: {.margin-column}

[^assignop]: An assignment operator is used to assign data values to objects stored in the work space. The commonly used `<-` is equivalent to `=`. Note however that `->` is different from `=`.  

:::

The `filter` function keeps rows of a data frame 








## Scientific graphic design

Tufte [@tufte2001] provides us with several principles for creating effective data visualizations. 

-   Show the data

-   Maximize the data-ink ratio

-   Erase non-data ink

-   Erase redundant data ink

-   Revise and edit


### Chartjunk

See `ggpattern` etc for creating chart junk with ggplot2

### Minimizing

See `ggthemes` for range-frame as suggested in tufte Ch 6. Manually minimize summary displays such as boxplots...

### Labels and annotations

Using `ggtext` to format labels etc.

### Glamorous graphics

In a talk at RStudio conference in 2020, Will Chase presented principles related to glamorous graphics.

Why glamorous graphics? - Engage a wider audience - Aesthetics matters for understanding - "respecting the audience"

The glamour of graphics uses color, typography and layout to increase the impact of the graphic.

### Layout

#### Alignment

Top left align titles

Text should be horizontal

Axis labels should also be horizontal

Alignment should provide clean lines and symmetry

Some alignment can be left/center/right to provide overall structure to the graph.

#### Border, Grid lines, legends

Do not use borders or grid lines. If grid lines, make them lighter and more subtle.

Avoid legends, labels of the data are better or titles with color.

#### White space

Use white space to separate elements of the graph and give the reader room to understand the graph.

### Typography

#### Fonts

#### Hierarchy

This gives the reader a hierarchy of information.

### Color

What colors look good together?

```{r}

# Load required libraries
library(ggplot2)

# Create a data frame of angles and radii
angles <- seq(0, 360, by = 0.1)
radii <- seq(0.1, 1, by = 0.1)  # Vary the starting point to avoid a point at the center
df <- expand.grid(angle = angles, radius = radii)

# Create ggplot of colored points in polar coordinates
p <- ggplot(df, aes(x = angle, y = radius, color = angle, alpha = radius)) +
  geom_point(size = 8) +
  scale_color_gradientn(colors = rainbow(360)) +
  scale_alpha_continuous(range = c(1, 0.1)) +
  coord_polar(theta = "x") +
  theme_void() +
  theme(axis.text = element_blank(),
        axis.title = element_blank(),
        axis.ticks = element_blank(),
        legend.position = "none")
  

#### Alternative

# Create a data frame of angles from 0 to 360
angles <- seq(0, 360, by = 0.2)
df <- data.frame(angle = angles)

# Create ggplot of colored points in polar coordinates
p <- ggplot(df, aes(x = angle, y = 1, color = angle)) +
  geom_point(size = 10) +
  scale_color_gradientn(colors = rainbow(360)) +
  coord_polar(theta = "x") +
  theme_void() +
  theme(axis.text = element_blank(),
        axis.title = element_blank(),
        axis.ticks = element_blank(),
        legend.position = "none")

```

The color wheel can be used to find

-   Complementary colors that will give high contrast
-   Monochromatic will give harmonious\
-   Analogous are choosen from the same side on the color wheele colors to avoid contrast but still enable showing differences
-   Triadic colors to give more

<!-- SEE  https://www.canva.com/colors/color-wheel/ for more on this -->

#### Describing colors, Hue, Saturation and Lightness

#### Background colors

Colors can help reduce "hardness" by avoiding a white background.
